{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krish97na/Krish97na/blob/main/assignment_ecg_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z00pmDU9qUai"
      },
      "source": [
        "# WASP Course: Artificial Intelligence and Machine Learning\n",
        "\n",
        "Lecturer: Dave Zachariah\n",
        "\n",
        "Assignment responsible: Jingwei Hu, Tianru Zhang, David Vävinggren"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxpVN-H9DRkp"
      },
      "source": [
        "# Student and Group Information\n",
        "\n",
        "Fill this out for the submission of the assignment (you submit this notebook with your solution)\n",
        "\n",
        "- **Student names:** <font color='red'>Fill in</font>\n",
        "\n",
        "- **Team ID:** <font color='red'>Fill in</font>\n",
        "\n",
        "Make sure that the team id is the same as the one with which you submit your model predictions (see coding task 7) such that we can check your performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_X8-WK_Htgr"
      },
      "source": [
        "---\n",
        "# Module 3 - Assignment Overview: ECG classification\n",
        "\n",
        "The [electrocardiogram (ECG)](https://www.mayoclinic.org/tests-procedures/ekg/about/pac-20384983) records the electrical signals in the heart. It is a common  test used to quickly detect heart problems and to monitor the heart's health.\n",
        "In this assignment you will implement and evaluate a model to classify whether the person has [atrial fibrillation (AF)](https://www.mayoclinic.org/diseases-conditions/atrial-fibrillation/symptoms-causes/syc-20350624.) or not based on measurements from the ECG exam.\n",
        "\n",
        "\n",
        "**Submission:** You submit the deliverables (see below) at https://canvas.kth.se/courses/54581/assignments\n",
        "\n",
        "**Due Date:** August 22, 2025.\n",
        "\n",
        "---\n",
        "## Basic Tasks\n",
        "Your task is to implement a classification model, train this model on training data, and evaluate its performance on validation data. We provide skeleton code for the implementation of a simple convolution neural network model.\n",
        "\n",
        "The steps required to implement this model are presented as numbered tasks below. In total there are seven (7) coding tasks and five (5) explanation tasks.\n",
        "\n",
        "## Competitive setting\n",
        "\n",
        "You have to compute the predictions for the test data (you do not have the labels for it) and submit your predictions to be evaluated to a leaderboard. These predictions will be scored and your submission will be ranked according to the F1 score and compared with your colleagues. In the end a winning team will be determined.\n",
        "\n",
        "### Deliverables\n",
        "There are two deliverables:\n",
        "1. You have to submit this Jupyter notebook on the course web-page (Canvas) together with your code and explanations (where asked for it) that describe your implementation and your experimental results. The notebook should run as a standalone in google colab.\n",
        "2. You have to have at least **three (3)** submissions (for instructions on how to submit, see coding task 7) where you try to improve the model architecture, the training procedure or the problem formulation. In the submission of this notebook you have to provide a short explanation of what changed between each submission and justify why you decided to make these changes.\n",
        "\n",
        "### Grading\n",
        "To pass the assignment, you must submit a complete and working implementation of a model and a well-motivated description and evaluation of it. Your model should reach an Area under the ROC curve (AUROC) on the test data of at least 0.97 and an Average Precision (AP) score of 0.95. Note that the leaderboard to is sorted by F1 score and not AUROC, hence you would want to balance all three metrics.\n",
        "\n",
        "### GPU Acceleration\n",
        "To be able to use the GPUs provided by colab in order to speed up your computations, you want to check that the `Hardware accelerator` is set to `GPU` under `Runtime > change runtime type`. Note that notebooks run by connecting to virtual machines that have maximum lifetimes that can be as much as 12 hours. Notebooks will also disconnect from VMs when left idle for too long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fecRFQhGDRkq",
        "outputId": "255924f5-b539-4ce4-8a26-5eba274390a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'req'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# helper function\n",
        "def exists(path):\n",
        "    val = os.path.exists(path)\n",
        "    if val:\n",
        "        print(f'{path} already exits. Using cached. Delete it manually to recieve it again!')\n",
        "    return val\n",
        "\n",
        "# clone requirements.txt if not yet available\n",
        "if not exists('requirements.txt'):\n",
        "    !git clone https://gist.github.com/dgedon/8a7b91714568dc35d0527233e9ceada4.git req\n",
        "    !mv req/requirements.txt .\n",
        "    !yes | rm -r req"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RZ_o0wKcm7WM",
        "outputId": "60c4c129-174c-482a-b19e-860554509a48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting certifi==2023.5.7 (from -r requirements.txt (line 1))\n",
            "  Downloading certifi-2023.5.7-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting cffi==1.15.1 (from -r requirements.txt (line 2))\n",
            "  Downloading cffi-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting charset-normalizer==3.1.0 (from -r requirements.txt (line 3))\n",
            "  Downloading charset_normalizer-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\n",
            "Collecting colorama==0.4.6 (from -r requirements.txt (line 4))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting cycler==0.11.0 (from -r requirements.txt (line 5))\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting ecg-plot==0.2.8 (from -r requirements.txt (line 6))\n",
            "  Downloading ecg_plot-0.2.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting filelock==3.12.2 (from -r requirements.txt (line 7))\n",
            "  Downloading filelock-3.12.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting h5py==3.8.0 (from -r requirements.txt (line 8))\n",
            "  Downloading h5py-3.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting idna==3.4 (from -r requirements.txt (line 9))\n",
            "  Downloading idna-3.4-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting Jinja2==3.1.2 (from -r requirements.txt (line 10))\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting kiwisolver==1.4.4 (from -r requirements.txt (line 11))\n",
            "  Downloading kiwisolver-1.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting MarkupSafe==2.1.3 (from -r requirements.txt (line 12))\n",
            "  Downloading MarkupSafe-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting matplotlib==3.3.4 (from -r requirements.txt (line 13))\n",
            "  Downloading matplotlib-3.3.4.tar.gz (37.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (1.3.0)\n",
            "Collecting networkx==3.1 (from -r requirements.txt (line 15))\n",
            "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting numpy==1.25.0 (from -r requirements.txt (line 16))\n",
            "  Downloading numpy-1.25.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting pandas==2.0.2 (from -r requirements.txt (line 17))\n",
            "  Downloading pandas-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting Pillow==9.5.0 (from -r requirements.txt (line 18))\n",
            "  Downloading Pillow-9.5.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting pycparser==2.21 (from -r requirements.txt (line 19))\n",
            "  Downloading pycparser-2.21-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pyparsing==3.1.0 (from -r requirements.txt (line 20))\n",
            "  Downloading pyparsing-3.1.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-dateutil==2.8.2 (from -r requirements.txt (line 21))\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting pytz==2023.3 (from -r requirements.txt (line 22))\n",
            "  Downloading pytz-2023.3-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting requests==2.31.0 (from -r requirements.txt (line 23))\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting scipy==1.10.1 (from -r requirements.txt (line 24))\n",
            "  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting six==1.16.0 (from -r requirements.txt (line 25))\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting soundfile==0.12.1 (from -r requirements.txt (line 26))\n",
            "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl.metadata (14 kB)\n",
            "Collecting sympy==1.12 (from -r requirements.txt (line 27))\n",
            "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting torch==2.0.1 (from -r requirements.txt (line 28))\n",
            "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchvision==0.2.2.post3 (from -r requirements.txt (line 29))\n",
            "  Downloading torchvision-0.2.2.post3-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting tqdm==4.65.0 (from -r requirements.txt (line 30))\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing_extensions==4.6.3 (from -r requirements.txt (line 31))\n",
            "  Downloading typing_extensions-4.6.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting tzdata==2023.3 (from -r requirements.txt (line 32))\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting urllib3==2.0.3 (from -r requirements.txt (line 33))\n",
            "  Downloading urllib3-2.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting wfdb==4.1.2 (from -r requirements.txt (line 34))\n",
            "  Downloading wfdb-4.1.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting xmljson==0.2.1 (from -r requirements.txt (line 35))\n",
            "  Downloading xmljson-0.2.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 28))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 28))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r requirements.txt (line 28))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r requirements.txt (line 28))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r requirements.txt (line 28))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r requirements.txt (line 28))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r requirements.txt (line 28))\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r requirements.txt (line 28))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r requirements.txt (line 28))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r requirements.txt (line 28))\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r requirements.txt (line 28))\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1->-r requirements.txt (line 28))\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 28)) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 28)) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 28)) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 28))\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'torchvision' candidate (version 0.2.2.post3 at https://files.pythonhosted.org/packages/fb/01/03fd7e503c16b3dc262483e5555ad40974ab5da8b9879e164b56c1f4ef6f/torchvision-0.2.2.post3-py2.py3-none-any.whl (from https://pypi.org/simple/torchvision/))\n",
            "Reason for being yanked: So that users won't accidentally install this when using python 3.11\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.0/157.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cffi-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (462 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.6/462.6 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (197 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.3/197.3 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Downloading ecg_plot-0.2.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
            "Downloading h5py-3.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.25.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Pillow-9.5.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m848.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.2.2.post3-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.6.3-py3-none-any.whl (31 kB)\n",
            "Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wfdb-4.1.2-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmljson-0.2.1-py2.py3-none-any.whl (10 kB)\n",
            "Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: matplotlib\n",
            "  Building wheel for matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for matplotlib: filename=matplotlib-3.3.4-cp311-cp311-linux_x86_64.whl size=11676239 sha256=0631abd2d1a3d66598ec98dcabb260f00e87d9bbd06f2592285e70e0441a17e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/f5/4e/66/4bf0958840d0ff3311f8e597fef91ae567c88caca711278d5e\n",
            "Successfully built matplotlib\n",
            "Installing collected packages: xmljson, pytz, lit, ecg-plot, urllib3, tzdata, typing_extensions, tqdm, sympy, six, pyparsing, pycparser, Pillow, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, MarkupSafe, kiwisolver, idna, filelock, cycler, colorama, charset-normalizer, certifi, scipy, requests, python-dateutil, nvidia-cusolver-cu11, nvidia-cudnn-cu11, Jinja2, h5py, cffi, soundfile, pandas, matplotlib, wfdb, triton, torch, torchvision\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.14.1\n",
            "    Uninstalling typing_extensions-4.14.1:\n",
            "      Successfully uninstalled typing_extensions-4.14.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.3\n",
            "    Uninstalling pyparsing-3.2.3:\n",
            "      Successfully uninstalled pyparsing-3.2.3\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.22\n",
            "    Uninstalling pycparser-2.22:\n",
            "      Successfully uninstalled pycparser-2.22\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.2.1\n",
            "    Uninstalling pillow-11.2.1:\n",
            "      Successfully uninstalled pillow-11.2.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.5\n",
            "    Uninstalling networkx-3.5:\n",
            "      Successfully uninstalled networkx-3.5\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.8\n",
            "    Uninstalling kiwisolver-1.4.8:\n",
            "      Successfully uninstalled kiwisolver-1.4.8\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.2\n",
            "    Uninstalling charset-normalizer-3.4.2:\n",
            "      Successfully uninstalled charset-normalizer-3.4.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.7.9\n",
            "    Uninstalling certifi-2025.7.9:\n",
            "      Successfully uninstalled certifi-2025.7.9\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.14.0\n",
            "    Uninstalling h5py-3.14.0:\n",
            "      Successfully uninstalled h5py-3.14.0\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.17.1\n",
            "    Uninstalling cffi-1.17.1:\n",
            "      Successfully uninstalled cffi-1.17.1\n",
            "  Attempting uninstall: soundfile\n",
            "    Found existing installation: soundfile 0.13.1\n",
            "    Uninstalling soundfile-0.13.1:\n",
            "      Successfully uninstalled soundfile-0.13.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.2 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\n",
            "pydantic 2.11.7 requires typing-extensions>=4.12.2, but you have typing-extensions 4.6.3 which is incompatible.\n",
            "langchain-core 0.3.68 requires typing-extensions>=4.7, but you have typing-extensions 4.6.3 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires scipy>=1.11, but you have scipy 1.10.1 which is incompatible.\n",
            "typing-inspection 0.4.1 requires typing-extensions>=4.12.0, but you have typing-extensions 4.6.3 which is incompatible.\n",
            "bigframes 2.8.0 requires matplotlib>=3.7.1, but you have matplotlib 3.3.4 which is incompatible.\n",
            "curl-cffi 0.11.4 requires certifi>=2024.2.2, but you have certifi 2023.5.7 which is incompatible.\n",
            "blosc2 3.5.1 requires numpy>=1.26, but you have numpy 1.25.0 which is incompatible.\n",
            "typeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.6.3 which is incompatible.\n",
            "seaborn 0.13.2 requires matplotlib!=3.6.1,>=3.4, but you have matplotlib 3.3.4 which is incompatible.\n",
            "scikit-image 0.25.2 requires pillow>=10.1, but you have pillow 9.5.0 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.0.2 which is incompatible.\n",
            "google-genai 1.25.0 requires typing-extensions<5.0.0,>=4.11.0, but you have typing-extensions 4.6.3 which is incompatible.\n",
            "pygit2 1.18.0 requires cffi>=1.17.0, but you have cffi 1.15.1 which is incompatible.\n",
            "altair 5.5.0 requires typing-extensions>=4.10.0; python_version < \"3.14\", but you have typing-extensions 4.6.3 which is incompatible.\n",
            "fastai 2.7.19 requires torchvision>=0.11, but you have torchvision 0.2.2.post3 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 2.0.2 which is incompatible.\n",
            "dataproc-spark-connect 0.8.2 requires tqdm>=4.67, but you have tqdm 4.65.0 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.25.0 which is incompatible.\n",
            "openai 1.94.0 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.6.3 which is incompatible.\n",
            "pytensor 2.31.7 requires filelock>=3.15, but you have filelock 3.12.2 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\n",
            "plotnine 0.14.6 requires matplotlib>=3.8.0, but you have matplotlib 3.3.4 which is incompatible.\n",
            "plotnine 0.14.6 requires pandas>=2.2.0, but you have pandas 2.0.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.25.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.0 which is incompatible.\n",
            "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 3.1 which is incompatible.\n",
            "wandb 0.21.0 requires typing-extensions<5,>=4.8, but you have typing-extensions 4.6.3 which is incompatible.\n",
            "arviz 0.21.0 requires matplotlib>=3.5, but you have matplotlib 3.3.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires h5py>=3.11.0, but you have h5py 3.8.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.0 which is incompatible.\n",
            "cvxpy 1.6.6 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "fastapi 0.116.0 requires typing-extensions>=4.8.0, but you have typing-extensions 4.6.3 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Jinja2-3.1.2 MarkupSafe-2.1.3 Pillow-9.5.0 certifi-2023.5.7 cffi-1.15.1 charset-normalizer-3.1.0 colorama-0.4.6 cycler-0.11.0 ecg-plot-0.2.8 filelock-3.12.2 h5py-3.8.0 idna-3.4 kiwisolver-1.4.4 lit-18.1.8 matplotlib-3.3.4 networkx-3.1 numpy-1.25.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pandas-2.0.2 pycparser-2.21 pyparsing-3.1.0 python-dateutil-2.8.2 pytz-2023.3 requests-2.31.0 scipy-1.10.1 six-1.16.0 soundfile-0.12.1 sympy-1.12 torch-2.0.1 torchvision-0.2.2.post3 tqdm-4.65.0 triton-2.0.0 typing_extensions-4.6.3 tzdata-2023.3 urllib3-2.0.3 wfdb-4.1.2 xmljson-0.2.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "cycler",
                  "dateutil",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pyparsing",
                  "six"
                ]
              },
              "id": "bcaaa46cd14c4bb7a869d35a0ad480f7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install packages (python>=3.9 is required)\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JkjZfvuKHd6q"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import h5py\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTfxCz1YIpGP"
      },
      "source": [
        "---\n",
        "## The data set\n",
        "\n",
        "The dataset is a subset of the [*CODE dataset*](https://scilifelab.figshare.com/articles/dataset/CODE_dataset/15169716): an anotated database of ECGs. The ECG exams were recorded in Brazil by the Telehealth Network of the state Minas Gerais between 2010 and 2016. The dataset and its usage for the development of deep learning methods was described in [\"Automatic diagnosis of the 12-lead ECG using a deep neural network\"](https://www.nature.com/articles/s41467-020-15432-4).\n",
        "The full dataset is available for research upon request.\n",
        "\n",
        "\n",
        "For the training dataset you have labels.\n",
        "For the test dataset you only have the ECG exams but no labels. Evaluation is done by submitting to the leaderboard.\n",
        "\n",
        "Download the dataset from the given dropbox link and unzip the folder containing the files. The downloaded files are in WFDB format (see [here](https://www.physionet.org/content/wfdb-python/3.4.1/) for details)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeNTb95FM6R1"
      },
      "outputs": [],
      "source": [
        "# 1. Download dataset\n",
        "if not exists('codesubset.tar.gz'):\n",
        "    !wget https://www.dropbox.com/s/9zkqa5y5jqakdil/codesubset.tar.gz?dl=0 -O codesubset.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQR2EI49OVP0"
      },
      "outputs": [],
      "source": [
        "# 1. unzip the downloaded data set folder\n",
        "if not exists('codesubset'):\n",
        "    !tar -xf codesubset.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVFAJG66DRkr"
      },
      "source": [
        "Note that the extraced folder 'codesubset' contains\n",
        "1. subfolders with the ECG exam traces. These have to be further preprocessed which we do in the next steps.\n",
        "2. a csv file which contain the labels and other features for the training data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H8tsO-QABmw"
      },
      "source": [
        "\n",
        "### Preprocessing\n",
        "\n",
        "Run the cells below to  Clone the GitHub repository which we use for [data preprocessing](https://github.com/antonior92/ecg-preprocessing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yz7VW7nQEO-"
      },
      "outputs": [],
      "source": [
        "# 2. clone the code files for data preprocessing\n",
        "if not exists('ecg-preprocessing'):\n",
        "    !git clone https://github.com/paulhausner/ecg-preprocessing.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlvD4bEtQUhc"
      },
      "source": [
        "Let us plot an ECG sample. We can plot ECGs using the `ecg_plot` library for example by using the following code snippet where `ecg_sample` is an array of size `(number of leads * sequence length)`. Now we can view an ECG before preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOpX0vlEQVUU"
      },
      "outputs": [],
      "source": [
        "import ecg_plot\n",
        "runfile(\"ecg-preprocessing/read_ecg.py\")\n",
        "\n",
        "PATH_TO_WFDB = 'codesubset/train/TNMG100046'\n",
        "ecg_sample, sample_rate, _ = read_ecg(PATH_TO_WFDB)\n",
        "\n",
        "# ECG plot\n",
        "plt.figure()\n",
        "lead = ['I', 'II', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
        "ecg_plot.plot(ecg_sample, sample_rate=sample_rate, style='bw', row_height=8, lead_index=lead, columns=1, title='Sample ECG before pre-processing')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_icbSxgDowE"
      },
      "source": [
        "\n",
        "The preprocessing consist of:\n",
        "- resampling all ECG traces to the sample sampling period (400 Hz). Option: ``--new_freq 400``\n",
        "- zero padding if necessary such that all ECG have the same number of samples (4096). Option: ``--new_len 4096``.\n",
        "- removing trends in the ECG signal. Option: ``--remove_baseline``\n",
        "- remove possible power line noise. Option: ``--powerline 60``\n",
        "\n",
        "You can run the script bellow to plot the same ECG after the preprocessing.  The script also use the  `ecg_plot` library (as you did above).  You can try also with different command line options to see how the preprocessing affects the signal that will be used by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDQfCECcDoGN"
      },
      "outputs": [],
      "source": [
        "%run ecg-preprocessing/plot_from_ecg.py codesubset/train/TNMG100046 --new_freq 400 --new_len 4096 --remove_baseline --powerline 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2Kh8RkYD8bE"
      },
      "source": [
        "\n",
        "Next we perform the preprocessing in all exams and convert them into one single h5 file (see [here](https://www.h5py.org/#:~:text=The%20h5py%20package%20is%20a,they%20were%20real%20NumPy%20arrays.) for details about the format). The resulting h5 files contains the traces as arrays with the shape `(number of traces * sequence length * number of leads)` where sequence length is 4096 and number of leads is 8.\n",
        "The files `train.h5` and `test.h5` will be saved inside the folder `codesubset/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyS0WXTzQU5c"
      },
      "outputs": [],
      "source": [
        "# 3. Generate train\n",
        "if not exists('codesubset/train.h5'):\n",
        "    !python ecg-preprocessing/generate_h5.py --new_freq 400 --new_len 4096 --remove_baseline --powerline 60 codesubset/train/RECORDS.txt codesubset/train.h5\n",
        "# 3. Generate test\n",
        "if not exists('codesubset/test.h5'):\n",
        "    !python ecg-preprocessing/generate_h5.py --new_freq 400 --new_len 4096 --remove_baseline --powerline 60 codesubset/test/RECORDS.txt codesubset/test.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvelAs8NJExH"
      },
      "source": [
        "### Coding Task 1: Data Analysis\n",
        "\n",
        "Before starting to model you have to analyse the dataset. You can be creative in your way of *getting a feeling* for the data. What you have to do is:\n",
        "- plot an ECG after proprocessing saved in the hdf5 file. For this use the `ecg_plot()` example above and see below for how to access the preprocessed data in h5 format.\n",
        "\n",
        "Some further ideas to explore are:\n",
        "- check the balance of the data set,\n",
        "- evaluate the distribution of age and sex of the patients,\n",
        "- think about the performance that a best naive classifier would achieve, e.g. by random guessing or always predicting one class.\n",
        "\n",
        "<br />\n",
        "\n",
        "**How to access the data?**\n",
        "\n",
        "You can acces the data in the h5 file in the following way\n",
        "```\n",
        "import h5py\n",
        "\n",
        "PATH_TO_H5_FILE = 'codesubset/train.h5'\n",
        "f = h5py.File(PATH_TO_H5_FILE, 'r')\n",
        "data = f['tracings']\n",
        "```\n",
        "Then, `data[i]` is an numpy array of the $i$th ECG exam (including all time points and leads).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja8j1xOYJdqg"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "TASK: Insert your code here\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu4tpfc3STcD"
      },
      "source": [
        "### Explanation task 1: Data Analysis\n",
        "\n",
        "Please explain your main findings of the data analysis task in a few bullet points. Explain also what the preprocessing does and why it is necessary.\n",
        "\n",
        "<br />\n",
        "\n",
        "\n",
        "**<font color='red'>Your explanation here:</font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO_E8qGUJ2Db"
      },
      "source": [
        "---\n",
        "## Model\n",
        "\n",
        "The model class consists of two methods:\n",
        "- `__init__(self, args)`: This methods initializes the class, e.g. by using `mymodel=ModelBaseline(args)`.\n",
        "- `forward(self,input_data)`: This method is called when we run `model_output=mymodel(input_data)`.\n",
        "\n",
        "The dimension of the input data is  `(batch size * sequence length * number of leads)`. Where **batch size** is a hyperparameter, **sequence length** is the number of ECG time samples (=4096) and **number of leads** (=8).\n",
        "\n",
        "The `ModelBaseline` (provided below) is a 2 layer model with one convolutional layers and one linear layer. Some explanations:\n",
        "- The conv layer downsamples the input traces from 4096 samples to 128 samples and increases the number of channels from 8 (=number of leads) to 32. Here we use a kernel size of 3.\n",
        "- The linear layer uses the flattened output from the conv and outputs one prediction. Since we have a binary problem, a single prediction is sufficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pONc-F25K-Z5"
      },
      "outputs": [],
      "source": [
        "class ModelBaseline(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(ModelBaseline, self).__init__()\n",
        "        self.kernel_size = 3\n",
        "\n",
        "        # conv layer\n",
        "        downsample = self._downsample(4096, 128)\n",
        "        self.conv1 = nn.Conv1d(in_channels=8,\n",
        "                               out_channels=32,\n",
        "                               kernel_size=self.kernel_size,\n",
        "                               stride=downsample,\n",
        "                               padding=self._padding(downsample),\n",
        "                               bias=False)\n",
        "\n",
        "        # linear layer\n",
        "        self.lin = nn.Linear(in_features=32*128,\n",
        "                             out_features=1)\n",
        "\n",
        "        # ReLU\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def _padding(self, downsample):\n",
        "        return max(0, int(np.floor((self.kernel_size - downsample + 1) / 2)))\n",
        "\n",
        "    def _downsample(self, seq_len_in, seq_len_out):\n",
        "        return int(seq_len_in // seq_len_out)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= x.transpose(2,1)\n",
        "\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x_flat= x.view(x.size(0), -1)\n",
        "        x = self.lin(x_flat)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q5oKHb1Ls87"
      },
      "source": [
        "### Coding Task 2: Define your model\n",
        "\n",
        "In the cell below you have to define your model. You can be inspired by the baseline model above but you can also define any other kind of neural network architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BHovxZZLvkd"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "    def forward(x):\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7besXJ1Qjax"
      },
      "source": [
        "### Explanation Task 2: Final Model\n",
        "Please explain and motivate in short sentences or bullet points the choice of your final model.\n",
        "\n",
        "<br />\n",
        "\n",
        "\n",
        "**<font color='red'>Your explanation here:</font>**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeWVnhT1L72u"
      },
      "source": [
        "---\n",
        "## Train function\n",
        "\n",
        "The function `train(...)` is called to in every epoch to train the model. The function loads the training data, makes predictions, compares predictions with true labels in the loss function and adapting the model parameters using stochastic gradient descent.\n",
        "\n",
        "In the code cell below there is the basic structure to load data from the data loader and to log your loss. The arguments of the function are explained by the use in the `main(...)` function below.\n",
        "\n",
        "If you are unfamiliar with PyTorch training loops, then this official [tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) might help (especially section \"4. Train your Network\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHcflxJ1Wprw"
      },
      "source": [
        "### Coding Task 3: Fill training loop\n",
        "\n",
        "Fill the code cell below such that the model is training when `train(...)` is called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMLCx9Cahr7f"
      },
      "outputs": [],
      "source": [
        "def train_loop(epoch, dataloader, model, optimizer, loss_function, device):\n",
        "    # model to training mode (important to correctly handle dropout or batchnorm layers)\n",
        "    model.train()\n",
        "    # allocation\n",
        "    total_loss = 0  # accumulated loss\n",
        "    n_entries = 0   # accumulated number of data points\n",
        "    # progress bar def\n",
        "    train_pbar = tqdm(dataloader, desc=\"Training Epoch {epoch:2d}\".format(epoch=epoch), leave=True)\n",
        "    # training loop\n",
        "    for traces, diagnoses in train_pbar:\n",
        "        # data to device (CPU or GPU if available)\n",
        "        traces, diagnoses = traces.to(device), diagnoses.to(device)\n",
        "\n",
        "        \"\"\"\n",
        "        TASK: Insert your code here. This task can be done in 5 lines of code.\n",
        "        \"\"\"\n",
        "\n",
        "        # Update accumulated values\n",
        "        total_loss += loss.detach().cpu().numpy()\n",
        "        n_entries += len(traces)\n",
        "\n",
        "        # Update progress bar\n",
        "        train_pbar.set_postfix({'loss': total_loss / n_entries})\n",
        "    train_pbar.close()\n",
        "    return total_loss / n_entries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQnstiLiWB1"
      },
      "source": [
        "---\n",
        "## Eval function\n",
        "\n",
        "The `eval(...)` function is similar to the `train(...)` function but is used to evaluate the model on validation data without adapting the model parameters. You can prohibit computing gradients by using a `with torch.no_grad():` statement.\n",
        "\n",
        "Currenlty only the loss is logged here. Additionally you have to collect all your predictions and the true values in order to compute more metrics such as AUROC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22G-f_ooWunl"
      },
      "source": [
        "### Coding Task 4: Fill evaluation loop\n",
        "Fill the code cell below such we obtain model predictions to evaluate the validation loss and collect the predictoin in order to compute other validation metrics in the `main(...)` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWOvM9e5ijqk"
      },
      "outputs": [],
      "source": [
        "def eval_loop(epoch, dataloader, model, loss_function, device):\n",
        "    # model to evaluation mode (important to correctly handle dropout or batchnorm layers)\n",
        "    model.eval()\n",
        "    # allocation\n",
        "    total_loss = 0  # accumulated loss\n",
        "    n_entries = 0   # accumulated number of data points\n",
        "    valid_probs = []  # accumulated predicted probabilities\n",
        "    valid_true = [] # accumulated true labels\n",
        "\n",
        "    # progress bar def\n",
        "    eval_pbar = tqdm(dataloader, desc=\"Evaluation Epoch {epoch:2d}\".format(epoch=epoch), leave=True)\n",
        "    # evaluation loop\n",
        "    for traces_cpu, diagnoses_cpu in eval_pbar:\n",
        "        # data to device (CPU or GPU if available)\n",
        "        traces, diagnoses = traces_cpu.to(device), diagnoses_cpu.to(device)\n",
        "\n",
        "        \"\"\"\n",
        "        TASK: Insert your code here. This task can be done in 6 lines of code.\n",
        "        \"\"\"\n",
        "\n",
        "        # Update accumulated values\n",
        "        total_loss += loss.detach().cpu().numpy()\n",
        "        n_entries += len(traces)\n",
        "\n",
        "        # Update progress bar\n",
        "        eval_pbar.set_postfix({'loss': total_loss / n_entries})\n",
        "    eval_pbar.close()\n",
        "    return total_loss / n_entries, np.vstack(valid_probs), np.vstack(valid_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtBEPHo7jEZP"
      },
      "source": [
        "---\n",
        "## Run Training\n",
        "\n",
        "In the code cell below there are some initial (non-optimal!) training hyperparameters. Further, we combine everything from above into training code. That means that we build the dataloaders, define the model/loss/optimizer and then train/validate the model over multiple epochs. Here, we save the model with the lowest validation loss as the best model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Eh5IsvQWy0y"
      },
      "source": [
        "### Coding Task 5: Combine everything to train/validate the model\n",
        "\n",
        "The following tasks are necessary in the code below\n",
        "- split the data into training and validation data\n",
        "- define the loss function\n",
        "- decide and implement validation metric(s) to evaluate and compare the model on\n",
        "\n",
        "Optional task:\n",
        "- include learning rate scheduler\n",
        "- take specific care about possible data inbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOz48rnxdGfp"
      },
      "source": [
        "### Coding Task 6: Run your model and adapt hyperparameters\n",
        "\n",
        "After you combined everything in task 5, now you run the code to evaluate the model. Based on the resulting validation metrics you tune\n",
        "- the training hyperparameters\n",
        "- the model architecture\n",
        "- the model hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjGcqXpOdSbA"
      },
      "source": [
        "### Explanation Task 3: Hyperparameter\n",
        "Please explain and motivate in short sentences or bullet points the final choice of hyperparamer and how you developed them.\n",
        "\n",
        "<br />\n",
        "\n",
        "\n",
        "**<font color='red'>Your explanation here:</font>**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VNfdwn8IqQQ"
      },
      "outputs": [],
      "source": [
        "# set seed\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# choose variables\n",
        "\"\"\"\n",
        "TASK: Adapt the following hyperparameters if necessary\n",
        "\"\"\"\n",
        "learning_rate = 1e-2\n",
        "weight_decay = 1e-1\n",
        "num_epochs = 15\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQvRyaQcyvcM"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tqdm.write(\"Use device: {device:}\\n\".format(device=device))\n",
        "\n",
        "# =============== Build data loaders ======================================#\n",
        "tqdm.write(\"Building data loaders...\")\n",
        "\n",
        "path_to_h5_train, path_to_csv_train, path_to_records = 'codesubset/train.h5', 'codesubset/train.csv', 'codesubset/train/RECORDS.txt'\n",
        "# load traces\n",
        "traces = torch.tensor(h5py.File(path_to_h5_train, 'r')['tracings'][()], dtype=torch.float32)\n",
        "# load labels\n",
        "ids_traces = [int(x.split('TNMG')[1]) for x in list(pd.read_csv(path_to_records, header=None)[0])] # Get order of ids in traces\n",
        "df = pd.read_csv(path_to_csv_train)\n",
        "df.set_index('id_exam', inplace=True)\n",
        "df = df.reindex(ids_traces) # make sure the order is the same\n",
        "labels = torch.tensor(np.array(df['AF']), dtype=torch.float32).reshape(-1,1)\n",
        "# load dataset\n",
        "dataset = TensorDataset(traces, labels)\n",
        "len_dataset = len(dataset)\n",
        "n_classes = len(torch.unique(labels))\n",
        "# split data\n",
        "\"\"\"\n",
        "TASK: Split the dataset in train and validation; Insert your code here.\n",
        "This can be done in <=4 line of code\n",
        "\"\"\"\n",
        "\n",
        "# build data loaders\n",
        "train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)\n",
        "tqdm.write(\"Done!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMOKrUn9jfca"
      },
      "outputs": [],
      "source": [
        "# =============== Define model ============================================#\n",
        "tqdm.write(\"Define model...\")\n",
        "\"\"\"\n",
        "TASK: Replace the baseline model with your model; Insert your code here\n",
        "\"\"\"\n",
        "model = ModelBaseline()\n",
        "model.to(device=device)\n",
        "tqdm.write(\"Done!\\n\")\n",
        "\n",
        "# =============== Define loss function ====================================#\n",
        "\"\"\"\n",
        "TASK: define the loss; Insert your code here. This can be done in 1 line of code\n",
        "\"\"\"\n",
        "\n",
        "# =============== Define optimizer ========================================#\n",
        "tqdm.write(\"Define optimiser...\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "tqdm.write(\"Done!\\n\")\n",
        "\n",
        "# =============== Define lr scheduler =====================================#\n",
        "# TODO advanced students (non mandatory)\n",
        "\"\"\"\n",
        "OPTIONAL: define a learning rate scheduler; Insert your code here\n",
        "\"\"\"\n",
        "lr_scheduler = None\n",
        "\n",
        "# =============== Train model =============================================#\n",
        "tqdm.write(\"Training...\")\n",
        "best_loss = np.Inf\n",
        "# allocation\n",
        "train_loss_all, valid_loss_all = [], []\n",
        "\n",
        "# loop over epochs\n",
        "for epoch in trange(1, num_epochs + 1):\n",
        "    # training loop\n",
        "    train_loss = train_loop(epoch, train_dataloader, model, optimizer, loss_function, device)\n",
        "    # validation loop\n",
        "    valid_loss, y_pred, y_true = eval_loop(epoch, valid_dataloader, model, loss_function, device)\n",
        "\n",
        "    # collect losses\n",
        "    train_loss_all.append(train_loss)\n",
        "    valid_loss_all.append(valid_loss)\n",
        "\n",
        "    # compute validation metrics for performance evaluation\n",
        "    \"\"\"\n",
        "    TASK: compute validation metrics (e.g. AUROC); Insert your code here\n",
        "    This can be done e.g. in 5 lines of code\n",
        "    \"\"\"\n",
        "\n",
        "    # save best model: here we save the model only for the lowest validation loss\n",
        "    if valid_loss < best_loss:\n",
        "        # Save model parameters\n",
        "        torch.save({'model': model.state_dict()}, 'model.pth')\n",
        "        # Update best validation loss\n",
        "        best_loss = valid_loss\n",
        "        # statement\n",
        "        model_save_state = \"Best model -> saved\"\n",
        "    else:\n",
        "        model_save_state = \"\"\n",
        "\n",
        "    # Print message\n",
        "    tqdm.write('Epoch {epoch:2d}: \\t'\n",
        "                'Train Loss {train_loss:.6f} \\t'\n",
        "                'Valid Loss {valid_loss:.6f} \\t'\n",
        "                '{model_save}'\n",
        "                .format(epoch=epoch,\n",
        "                        train_loss=train_loss,\n",
        "                        valid_loss=valid_loss,\n",
        "                        model_save=model_save_state)\n",
        "                    )\n",
        "\n",
        "    # Update learning rate with lr-scheduler\n",
        "    if lr_scheduler:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "\"\"\"\n",
        "TASK: Here it can make sense to plot your learning curve; Insert your code here\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ouhd4vgt4jlS"
      },
      "source": [
        "---\n",
        "## Model Testing\n",
        "\n",
        "Since we saved our best model, we can now load the trained model and make predictions on the test data set. We save the predictions in a csv file which will be uploaded as part of the deliverables. Note that we take a `Sigmoid()` function on the model prediction in order to obtain soft predictions (probabilities) instead of hard predictions (0s or 1s)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXJRnGTpW7Qv"
      },
      "source": [
        "### Coding Task 7: Make prediction for test data\n",
        "\n",
        "Here you do not really need to code but you have to:\n",
        "- replace the baseline model with your model. If you do not use colab then change the path to the model location to load the trained model)\n",
        "- run the script. The predictions are saved in the variable `soft_pred`.\n",
        "- upload your predictions to the leaderboard online (see instruction details below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoidN7PLDRk0"
      },
      "outputs": [],
      "source": [
        "# build the dataloader once and re-use when running the cell below possibly multiple times.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# =============== Build data loaders ==========================================#\n",
        "tqdm.write(\"Building data loaders...\")\n",
        "# load data\n",
        "path_to_h5_test, path_to_csv_test = 'codesubset/test.h5', 'codesubset/test.csv'\n",
        "traces = torch.tensor(h5py.File(path_to_h5_test, 'r')['tracings'][()], dtype=torch.float32)\n",
        "dataset = TensorDataset(traces)\n",
        "len_dataset = len(dataset)\n",
        "# build data loaders\n",
        "test_dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "tqdm.write(\"Done!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-m4CYeW_hvO"
      },
      "outputs": [],
      "source": [
        "# =============== Define model ================================================#\n",
        "tqdm.write(\"Define model...\")\n",
        "\"\"\"\n",
        "TASK: Replace the baseline model with your model; Insert your code here\n",
        "\"\"\"\n",
        "model = ModelBaseline()\n",
        "\n",
        "# load stored model parameters\n",
        "ckpt = torch.load('model.pth', map_location=lambda storage, loc: storage)\n",
        "model.load_state_dict(ckpt['model'])\n",
        "# put model on device\n",
        "model.to(device=device)\n",
        "tqdm.write(\"Done!\\n\")\n",
        "\n",
        "# =============== Evaluate model ==============================================#\n",
        "model.eval()\n",
        "# allocation\n",
        "test_pred = torch.zeros(len_dataset,1)\n",
        "# progress bar def\n",
        "test_pbar = tqdm(test_dataloader, desc=\"Testing\")\n",
        "# evaluation loop\n",
        "end=0\n",
        "for traces in test_pbar:\n",
        "    # data to device\n",
        "    traces = traces[0].to(device)\n",
        "    start = end\n",
        "    with torch.no_grad():\n",
        "        # Forward pass\n",
        "        model_output = model(traces)\n",
        "\n",
        "        # store output\n",
        "        end = min(start + len(model_output), test_pred.shape[0])\n",
        "        test_pred[start:end] = torch.nn.Sigmoid()(model_output).detach().cpu()\n",
        "\n",
        "test_pbar.close()\n",
        "\n",
        "# =============== Save predictions ============================================#\n",
        "soft_pred = np.stack((1-test_pred.numpy(), test_pred.numpy()),axis=1).squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMxVJxHsgGdT"
      },
      "source": [
        "To upload your predictions to the leaderboard, use the following code. There are the following steps to follow:\n",
        "1. Download the GitHub repository for the leaderboard submission system.\n",
        "2. Register your team with a **team id** and **password**. The password ensures that only your team can upload to your team id. Do only run the registration once.\n",
        "3. Upload you predictions as a new submission. There are some things to obey here:\n",
        "    - For each submission you have to attach a note for you to keep track of the submission in the leaderboard and for us to know which submission you refer to in your explanation. Choose something meaningful such as \"submission A\" or \"model B\".\n",
        "    - You can only get one prediction evaluated per day and you get the score the following day. If you do multiple submissions on the same day, the initial submission will be overwritten and thus only the final submission will be evaluated.\n",
        "    - Only a maximum of ***FIVE*** submissions will be evaluated. So make them count! (If you update an submission before it is evaluated it doesn't count)\n",
        "    - The evaluation score is published with you team_id and note at http://hyperion.it.uu.se:5050/leaderboard\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BPWcP20RHwu"
      },
      "outputs": [],
      "source": [
        "# 1. Download repository for leaderboard submission system\n",
        "if not exists('leaderboard'):\n",
        "    !git clone https://gist.github.com/3ff6c4c867331c0bf334301842d753c7.git leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRaCnCLsgHPg"
      },
      "outputs": [],
      "source": [
        "# 2. Registration of your team\n",
        "host = \"http://hyperion.it.uu.se:5050/\"\n",
        "runfile(\"leaderboard/leaderboard_helpers.py\")\n",
        "\n",
        "\"\"\"\n",
        "TASK: Decide for a team_id (max 20 chars) and password.\n",
        "Do not change this after you have registered your team\n",
        "\"\"\"\n",
        "team_id = '' #Fill in a string\n",
        "password = '' #Fill in a string\n",
        "\n",
        "# run the registration\n",
        "r = register_team(team_id, password)\n",
        "if (r.status_code == 201):\n",
        "    print(\"Team registered successfully! Good luck\")\n",
        "elif not (r.status_code == 200):\n",
        "    raise Exception(\"You can not change your password once created. If you need help, please contact the teachers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I03QEBiegF7_"
      },
      "outputs": [],
      "source": [
        "# 3. Upload the prediction as submission\n",
        "\n",
        "# Write a note about the training procedure so you can identify it in the leaderboard. e.g. 5 epochs, or First  (Max 20 characters)\n",
        "\"\"\"\n",
        "TASK: Add a note for you submission\n",
        "\"\"\"\n",
        "note = '' #Fill in a string\n",
        "\n",
        "# Submit the predictions to the leaderboard. Note, this also saves your submissions in your colab folder\n",
        "r = submit(team_id, password, soft_pred.tolist(), note)\n",
        "if r.status_code == 201:\n",
        "    print(\"Submission successful!\")\n",
        "elif r.status_code == 200:\n",
        "    print(\"Submission updated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTKe9X-zTt7f"
      },
      "source": [
        "### Explanation Task 4: Submissions\n",
        "One of the grading criteria are three submissions to the leaderboard. List the three main submissions in the table below and explain the main changes in your code for each submission.\n",
        "\n",
        "<br />\n",
        "\n",
        "\n",
        "**<font color='red'>Your explanation here:</font>**\n",
        "\n",
        "Your team id: **<font color='red'>Fill in</font>**\n",
        "\n",
        "| Submission note | Accuracy | F1 | AUC | AP | Submission description |\n",
        "| --------------- | -------- | -- | --  | -- | ---------------------- |\n",
        "|xxx              | 0        | 0  | 0   | 0  | desc                   |\n",
        "|xxx              | 0        | 0  | 0   | 0  | desc                   |\n",
        "|xxx              | 0        | 0  | 0   | 0  | desc                   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw-GKZ4fVDhD"
      },
      "source": [
        "### Explanation Task 5: Reflection on Metrics\n",
        "Your were asked to reach a certain value in AUC and AP while maximising F1 for the leaderboard position. Explain in bullet points what aspect each of the metrics covers and why it is important not to just focus on one metric. What can happen if you only focus on AUC for example?\n",
        "\n",
        "<br />\n",
        "\n",
        "**<font color='red'>Your explanation here:</font>**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "assignment_ecg_classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}